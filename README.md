<h1 align="center"> 
	üìå Pipeline lambda com AWS CDK
</h1>

> ### Todo este projeto foi instanciado no bootcamp de engenharia de dados da HOWBootcamps, ministrado pelo senior data engineer Andr√© Sionek.

## Falando um poco sobre a HOWBootcamps

A HOW √© uma empresa coltada para bootcamps ao vivo, pr√°ticos e de curta dura√ß√£o com facilitadores das principais startups do Brasil. Para conhecer o trabalho deles basta acessar o [Site](https://learn.howedu.com.br/todos).

## Bootcamp

No bootcamp de engenaria avan√ßado de dados foi proposto a constru√ß√£o de uma pipeline que nos proporcionaria o contato com a ingest√£o, tratamento e disponibilidade de dados. Como diferencial o bootcamp focou em entregar a pipeline com automa√ß√µes e testes automatizados para monitoramento de cada etapa.

Como desafio durante todo desafio foi criar uma arquitetura lambda comtemplando as melhores pr√°ticas para coloca-l√° em produ√ß√£o. Falamos acima sobre a **Arquitetura lambda** e para facilitar o seu entendimento vamos detalhar suas caracter√≠sticas.

    1. Contempla em sua pipeline fluxos em batch e Real-time ou near-real-time.

    2. Entregua esses resultados em uma mesma camada.

    3. S√£o processados em jobs diferentes

## A arquitetura

Agora que ja sabemos as caracter√≠sticas da pipeline esbo√ßamos a arquitetura que foi montada e realizada durante o bootcamp.

<center><img src='img/arquitetura_sionek.png'></center>

### Para construi-la vamos seguir o seguinte step-by-step:

    1.  CI/CD instanciado e monitorado por meio do GitHub Actions utilizando e criando worflows.
    2.  Instanciamento de uma banco de dados. 
    3.  Captura de dados com DMS. 
    4.  Coleta de dados por meio de uma stream de dados com o Kinisis. 
    5.  Ingest√£o de dados por meio de uma API orquestrada pelo Airflow. 
    6.  Cria√ß√£o das tr√™s camadas do datalake (bronze, silver, gold).
    7.  Processamento de dados via databricks(microservi√ßo de processamento baseado no framework spark). 
    8.  Catalogo de metadados com glue catalog. 
    9.  Sistema de queries com athena. 
    10. Implementa√ß√£o de um Data WareHouse com a utiliza√ß√£o do framework DBT para gest√£o de um MDW(Modern Data WareHhouse).
    11. Orquestra√ß√£o da pipeline com Airflow.

# Hands-ON


Para montar esta pipeline vamos dividir todo o processo em 3 etapas **(1)** Cria√ß√£o dos ambientes de trabalho, **(2)** Cria√ß√£o da infraestrutura e **(3)** Cria√ß√£o dos seus Jobs e orquestra√ß√£o.


## 1) Cria√ß√£o dos ambientes de trabalho

Caso voc√™ esteja trabalhando com mais de uma pessoa em um projeto, ou mesmo sozinho √© de extrema importancia **criar h√°bitos e boas pr√°ticas de produ√ß√£o**. Para isso a primeira delas √© a utiliza√ß√£o de um **ambiente de versionamento de c√≥digo**, no caso deste repositorio utilizamos o git como versionador local e o github como remoto.

Outra pr√°tica √© a cria√ß√£o de workflows que vai garantir a qualidade e veracidade de sua aplica√ß√£o. Esse workflow recebe o nome de CI ou *continouos integration*, e neste est√£o contidos diversos testes unit√°rios para valida√ß√£o de sua aplica√ß√£o. Validado a aplica√ß√£o vamos para o segundo round de teste. Essa parte recebe o nome de CD ou *continouos deployment*, e nesta parte vamos fazer  alguns testes de integra√ß√£o que v√£o validar como essa nova aplica√ß√£o est√° rodando em contato com um sistema ja aplicado.

Dessa maneira temos a nossa "pol√≠cia" criada para validar a nossa aplica√ß√£o. Agora podemos criar os nossos ambientes, estes ser√£o:
* **Production** : Que seria como a nossa aplica√ß√£o em execu√ß√£o, a utima e a mais est√°vel.
* **Staging** : Que seria a pr√≥xima vers√£o a ser executada mas ainda em testes.
* **Deployment** : Ambiente de cria√ß√£o de novas features sem afetar as ja validadas. Neste caso √© mais de trabalho pessoal.


## 2) Cria√ß√£o da infraestrutura

Com isso vamos criar pontualmente cada uma das ferramentas da pipeline. Gosto da analogia da constru√ß√£o de uma carro, onde voc√™ n√£o constroi o carro como primeira tarefa. Ou seja, precisa que as rodas, o motor a carca√ßa esta pronta antes de construir ele como um todo.

Dessa maneira, nesta etapa vamos criar os modulos como (1) o banco de dados, (2) o datalake.
## 3) Cria√ß√£o dos seus Jobs

Os jobs esta atralado aos trabalhos e os processamentos atralados a pipeline. Basicamente anexar as leis de neg√≥cio para a pipeline.

## 4) A Orquestra√ß√£o

Garantir as automa√ß√µes e o monitoramento de cada parte da pipeline de forma que o uso dela cada vez se torna ainda mais f√°cil. AL√©m de todo projeto esta atrelado a um formato de programa√ß√£o chamado desing pattern que facilita a implementa√ß√£o e uso.

# CDK

Como todo o projeto se passa na **AWS** utilizamos a sua toolkit de desenvolvimento pr√≥pio que permite gerenciar toda uma √∫nica pipelina por meio das stacks do cloud formation. √â necessario ter a `AWS CLI` instalada assim como o `node` e a biblioteca do `CDK` da sua liguagem de desenvolvimento escolhido para utilizar.

Falando um pouco sobre estrutura do CDK temos as seguintes informa√ß√µes:

O arquivo `cdk.json` diz ao CDK Toolkit como executar seu aplicativo.

Este projeto √© montado como um projeto Python padr√£o.  A inicializa√ß√£o
tamb√©m cria um virtualenv dentro deste projeto, armazenado sob o processo `.venv'.
diret√≥rio.  Para criar o virtualenv, sup√µe-se que haja um `python3`.
(ou `pyhton` para Windows) execut√°vel em seu caminho com acesso ao `venv`.
pacote. Se por qualquer raz√£o a cria√ß√£o autom√°tica do virtualenv falhar,
voc√™ pode criar o virtualenv manualmente.

Para criar manualmente um virtualenv em MacOS e Linux:

```
$ python3 -m venv .venv
```

Ap√≥s a conclus√£o do processo de inicializa√ß√£o e a cria√ß√£o do virtualenv, voc√™ pode usar o seguinte
passo para ativar seu virtualenv.

```
source $ .venv/bin/activar
```

Se voc√™ √© uma plataforma Windows, ativaria o virtualenv desta forma:

```
% .venv\Scripts\activar.bat
```

Uma vez que o virtualenv √© ativado, voc√™ pode instalar as depend√™ncias necess√°rias.

```
$ pip install -r requirements.txt
```

Neste ponto, voc√™ pode agora sintetizar o modelo de CloudFormation para este c√≥digo.

```
cdk synth $ cdk
```

Para adicionar depend√™ncias adicionais, por exemplo, outras bibliotecas CDK, basta adicionar
para seu arquivo `setup.py` e reexecutar o arquivo `pip install -r requirements.txt`.
comando.

## Comandos √∫teis

 * `cdk ls` lista todas as stacks no aplicativo
 * `cdk synth` emite o modelo de CloudFormation synthesized
 * `cdk deploy` desdobre esta pilha para sua conta/regi√£o AWS padr√£o
 * `cdk diff` compara a pilha implantada com o estado atual
 * `cdk docs` documenta√ß√£o CDK aberta

---

<h1 align="center"> 
	üìå Refer√™ncias
</h1>